# Тестовое задание для Python разработчика

## Задача

Создать приложение для сбора и выдачи по запросу данных произвольных страниц в интернете.

Данные страницы включают тело (`HTML`), адрес (`URL`), заголовок (`title`).

Входные параметры для сбора данных со страницы в интернете:

- Адрес страницы.
- Максимальная глубина обхода.
- Максимальное количество одновременно загружаемых страниц.

Примечание о глубине обхода:

- глубина 0: сохранить данные только переданного адреса страницы;
- глубина N: сохранить данные всех страниц, на которые есть ссылки на страницах глубины `N - 1`.

Требуется возможность получения данных (HTTP API, Swagger UI):

1. Список страниц в виде их адресов и заголовков с возможностью поиска по обоим полям.
2. Тело конкретной страницы по её адресу.

## Требования к реализации

1. Актуальная версия Python.
2. Весь ввод-вывод асинхронный.
3. Запуск в Docker.
4. Оформление в виде Git-репозитория на GitHub.

Других требований нет. В частности, можно использовать любые БД, фреймворки, библиотеки.

## Критерии оценки

1. Выполнение требований и работоспособность приложения.
2. Полнота учтённых проблем, граничных случаев.
3. Общая архитектура кода и дизайн каждого класса и модуля в отдельности.
4. Стиль кода: читаемость, понятность, консистентное форматирование.
